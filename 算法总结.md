# 面试
## 比赛相关
### 一、语义分割比赛
#### deeplabv3发展历程
1. deeplabv1
    - 采用vgg16前向计算，减少了后两个池化层以减少图像下采样，为保证视野采用空洞卷积
    - 使用1x1卷积替换全连接层实现全卷积网络
    - 使用条件随机场使得边缘更清晰
    - 多尺寸预测，在每个池化层后面都加一层1x1卷积，类似图像金字塔结构（FCN），得到四个特征图作为预测。
2. deeplabv2
    - 受SPP启发的来ASPP，在最后采用类似inception的结构网络，多分支多样空洞卷积。
3. deeplabv3
    - 使用Multi-Grid策略，即在模型后端多家几层不同rate的空洞卷积，扩大感受野
    - 将batch归一化加入ASPP模块，使得数据训练更加有效
    - DeepLabv3 的 ASPP 加入了 全局池化层+conv1x1+双线性插值上采样 的模块 解决随着特征图的变小，空洞卷积退化为1x1卷积。
4. deeplabv3+
    - 将deeplabv3看成Encoder-Deconder结构
    - 借鉴MobileNet简化卷积
    - 使用修改过的Xception
#### 医学中Unet和Deeplabv3对比
相比于FCN和Deeplab等，UNet共进行了4次上采样，并在同一个stage使用了skip connection，而不是直接在高级语义特征上进行监督和loss反传，这样就保证了最后恢复出来的特征图融合了更多的low-level的feature，也使得不同scale的feature得到了的融合，从而可以进行多尺度预测和DeepSupervision。4次上采样也使得分割图恢复边缘等信息更加精细。

#### 语义分割loss
1. 交叉熵loss
2. 带权交叉熵loss
3. focal loss：解决样本数量不平衡，在二元交叉熵损失前面加参数a的基础上，将高置信度样本的损失降低一些，即动态a
4. Dice Loss：类似交并比结构，使用在样本极度不均衡的情况，如果一般情况下使用Dice Loss会回反向传播有不利的影响，使得训练不稳定。
5. IOU loss：与Dice loss类似，分母不同，略小。
6. Lovasz-Softmax Loss：


## 风控算法相关问题
参考链接：https://zhuanlan.zhihu.com/p/56175215
### 一、算法
#### 1、逻辑回归
 - 优点：
   - 结构：实现简单、速度快、占用内存小、可在短时间内迭代多个版本的模型。
   - 可解释性：模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是逻辑回归模型。
   - 鲁棒性：模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。
   - 特征工程：特征工程做得好，模型的效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。
   - 可用性：模型的结果可以很方便的转化为策略规则，且线上部署简单。
 - 缺点：
   - 准确度：容易欠拟合，相比集成模型，准确度不是很高。
   - 数据处理困难：对数据的要求比较高，逻辑回归对缺失值，异常值，共线性都比较敏感，且不能直接处理非线性的特征。所以在数据清洗和特征工程上会花去很大部分的时间。
   - 场景适应能力差：在金融领域对场景的适应能力有局限性，例如数据不平衡问题，高维特征，大量多类特征，逻辑回归在这方面不如决策树适应能力强。
 - 逻辑回归与线性回归：逻辑回归是一种广义线性模型，它引入了Sigmod函数，是非线性模型。**逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量因变量关系进行拟合。**
 - 过拟合问题：减少特征数量、正则化。
 - 逻辑回归为什么要对特征进行离散化：
   - 特征离散化：将连续特征通过分箱等操作进行离散，使用逻辑回归计算
   - 特征交叉：将不同特征通过计算形成合成特征，有助于表示非线性关系。
   - 优点：简化模型、降低过拟合、异常值处理、计算迅速易于迭代、离散特征交叉引入非线性。
 - 评分卡为什么要进行woe化：更好的解释行，避免one-hot编码隔离了原始数据间的关系；用于筛选特征；可处理缺失值。
 - 剔除高度相关特征：高度相关特征无用、简化模型、防止过拟合、提高可解释性。
 - 特征系数并不能表示重要性，特征相关性越高，系数表示重要性越不准确。
 - 为什么用极大似然函数作为损失函数：在逻辑回归中，极大似然函数是凸函数，更易于算法求解，如梯度下降、牛顿法等。

#### 2、决策树
 - 优点：
   - 可理解：易于理解，决策树可以生成IF..TEHN逻辑表达的树结构，可解释性很好。
   - 数据处理少：相比逻辑回归对数据的处理较简单，不太需要做例如数据离散化，归一化等操作。
   - 非线性：决策树是目前已知的对于处理非线性交互的最好的算法。
   - 效果好：模型的效果比较好，例如随机森林，xgboost都是基于决策树构建的。
 - 缺点：
   - 过拟合
   - 不适合高维数据
   - 泛化能力差，特别对于没出现过的值
 - 决策树原理：基于树结构构建，过程为：特征选择、决策树生成、剪枝。
 - 简述ID3，C4.5，CART三类决策树的原理和异同点：
   - ID3选择最佳分割点是基于信息增益的，信息增益越大，表明使用这个属性来划分所获得的“纯度提升”越大。C4.5对ID3进行了改进，因为ID3使用的信息增益对数据划分时，可能出现每个结点只包含一个样本，这些子节点的纯度已经达到最大，但是，这样的决策树并不具有泛化能力，无法对新样本进行预测。且ID3不能处理连续型变量和缺失值。而C4.5使用信息增益率来选择属性，克服了信息增益选择属性时偏向选择值多的属性的不足。且可以处理连续型变量和缺失值。
   - C4.5是基于ID3的改进版，只能用于分类。而CART树既可以做分类，也可以做回归。CART的本质是对特征空间进行二元划分，所以**CART生成的是一颗二叉树**，且可以对类别型变量和数值型变量进行分裂。对分类型变量进行划分时，分为等于该属性和不等于该属性，在对连续型变量进行划分时，分为大于和小于，在做分类时使用的是**GINI系数**作为划分标准，在做回归时使用的是**均方误差**。
 - 分类树和回归树的区别：回归树的流程是类似分类树的，**区别在于划分时的标准不再是最大熵，而是最小化均差**，如果节点的预测值错的越离谱，均方差越大，通过最小化均差能够找到最可靠的分支依据。
 - 决策树对缺失值如何处理：
   - 考虑选择属性时：忽略；统一值填充；根据缺失值给增益率打折（正则化）。
   - 选择后确定分割点：忽略；统一值填充；通过预测算法将缺失个体分配到子集中；全部分配给所有子集；作为单独分支。
   - 测试集上的缺失：分配给最常见的值；终止分类并归属给子集中概率最高的。
 - 决策树不需要归一化：决策树是一种概率模型，所以不需要做归一化，因为它不关心变量的值，而是关心变量的分布和变量之间的条件概率，所以归一化这种数值缩放，不影响分裂结点位置。
 - 过拟合问题：预剪枝；后剪枝。

#### 3、集成学习
 - 集成学习主要内容：
   - 集成学习是一种优化手段和策略，通常是结合多个简单的弱分类器来集成模型组，去做更可靠的决策。一般的弱分类器可以是决策树，SVM，kNN等构成，其中的模型可以单独来训练，并且这些弱分类器以某种方式结合在一起去做出一个总体预测。集成学习就是找出哪些弱分类器可以结合在一起，以及如何结合的方法。目前集成学习主要有bagging，boosting，stacking三种：
     - bagging：**对训练集进行随机子抽样，对每个子训练集构建基模型**，对所有的基模型的预测结果进行综合产生最后的预测结果。如果是分类算法，则用**多数投票法**确定最终类别，如果是回归算法，则将各个回归结果做**算术平均**作为最终的预测值。常用的bagging算法：随机森林
     - boosting：训练过程为**阶梯状**，基模型按照次序进行训练（实际上可以做到并行处理），先给定一个初始训练数据，训练出第一个基模型，根据基模型的表现对样本进行调整，在之前基模型**预测错误的样本（残差）**上投入更多的关注，然后用调整后的样本训练下一个基模型，重复上述过程N次，将N个基模型进行加权结合，输出最后的结果。常用的算法有GBDT，XGBOOST等。
     - stacking：是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练(一般用**LR**进行回归组合），从而得到完整的stacking模型。要得到stacking模型，关键在于如何构造第二层的特征，**构造第二层特征的原则是尽可能的避免信息泄露**，因此对原始训练集常常采用类似于K折交叉验证的划分方法。各个基模型要采用相同的Kfold，这样得到的第二层特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。
 - 偏差与方差：**偏差**描述的是**预测值与真实值的差距**，偏差越大，越偏离真实数据；**方差**描述的是**预测值的变化范围，离散程度，方差越大，数据分布越分散**；bagging主要关注的是降低方差，boosting主要关注降低偏差。

##### 随机森林
 - 原理：随机森林是bagging算法的代表，使用了CART树作为弱分类器，将多个不同的决策树进行组合。随机森林会通过**自助采样的方法（bootstrap）得到N个训练集**，然后在单个训练集上会**随机选择一部分特征**，来选择一个最优特征来做决策树的左右子树划分，最后得到N棵决策树，对于分类问题，按**多数投票**的准则确定最终结果，对于回归问题，由多棵决策树的预测值的**平均数**作为最终结果。随机森林的随机性体现在两方面，一个是**选取样本的随机性**，一个是**选取特征的随机性**，这样进一步增强了模型的泛化能力。
 - 优点：
   - 并行：训练可以高度并行化，训练速度快，效率高，随机性也减少了数据训练和特征选择的开销。
   - 泛化：两个随机性的引入，使得随机森林不容易过拟合，具有很好的抗噪声能力。
   - 数据适应性：对于数据的适应能力强，可以处理连续型和离散型的变量，数据无需规范化。
   - 作为降维方法：可以输出变量的重要程度，被认为是一种不错的降维方法。
 - 缺点：
   - 噪音不适：在某些噪声较大的分类问题和或回归问题上容易过拟合。
   - 可解释性：模型的可解释性比较差，无法控制模型内部的运行。
   - 低维数据不友好：对于小数据或者低维数据，效果可能会不太好。
 - 随机森林如何输出特征重要性：
   - 随机森林对于特征重要性的评估思想：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是**基尼指数**或**袋外数据错误率**。

##### Adaboost
 - 原理：Adaboost基于分类器的错误率分配不同的权重系数，最后得到累加加权的的预测结果。
   - 流程
     - 给数据中每一个样本一个权重，若有N个样本，则每个样本的权重为1/N.
     - 训练数据的每一个样本，得到第一个分类器。
     - 计算该分类器的错误率，**根据错误率计算给分类器分配的权重**。
     - 将第一个分类器**分错的样本权重增加，分对的样本权重减少**，然后再用新的样本权重训练数据，得到新的分类器。
     - 迭代这个训练步骤直到分类器错误为0或达到迭代次数。
     - 将所有的**弱分类器加权求和**，得到分类结果（分类器权重），错误率低的分类器获得更高的决定系数，从而在数据进行预测起关键作用。
   - 优点：
     - 精度高，构造简单，结果可理解
     - 可以使用各种回归分类模型来构建弱学习器，非常灵活。
     - 不容易过拟合。
   - 缺点：
     - 训练时会过于偏向分类困难的数据，导致Adaboost容易受噪声数据干扰。
     - 依赖于弱分类器，训练时间可能比较长。

##### GBDT
 - 原理：
   - GBDT是boosting的一种方法，主要思想是每一次建立单个分类器时，是在之前建立的模型的**损失函数的梯度下降方向**。损失函数越大，说明模型越容易出错，如果我们的模型能让损失函数持续的下降，则说明我们的模型在持续不断的改进，而最好的方式就是让损失函数在其梯度的方向上下降。
   - GBDT的核心在于**每一棵树学的是之前所有树结论和的残差**，残差就是真实值与预测值的差值，所以为了得到残差，GBDT中的树全部是**回归树**，之所以不用分类树，是因为分类的结果相减是没有意义的。
   - Shrinkage（缩减）是 GBDT 的一个重要演进分支，Shrinkage的思想在于每次走一小步来逼近真实的结果，要比直接迈一大步的方式更好（**慢逼近**），这样做可以有效**减少过拟合的风险**。它认为每棵树只学到了一小部分，累加的时候只累加这一小部分，通过多学习几棵树来弥补不足。这累加的一小部分（步长*残差）来逐步逼近目标，所以各个树的残差是渐变的而不是陡变的。
   - GBDT可以用于回归问题（线性和非线性），也可用于分类问题。
 - 高维稀疏特征不适用GBDT：GBDT在每次分割时需要大量特征，训练耗时；树的分割往往只考虑了少部分特征，大部分的特征都用不到，所有的高维稀疏的特征会造成大量的特征浪费。
 - GBDT与随机森林：
   - 相同点：都是由多棵树构成，最终的结果也是由多棵树决定。
   - 不同点：
     - 随机森林可以由分类树和回归树组成，GBDT只能由回归树组成。
     - 随机森林的树可以并行生成，而GBDT只能串行生成，所以随机森林的训练速度相对较快。
     - 随机森林关注减小模型的方差，GBDT关注减小模型的偏差。
     - 随机森林对异常值不敏感，GBDT对异常值非常敏感。
     - 随机森林最终的结果是多数投票或简单平均，而GBDT是加权累计起来。
 - 优点：
   - GBDT每一次的残差计算都增大了分错样本的权重，而分对的权重都趋近于0，因此泛化性能比较好。
   - 可以灵活的处理各种类型的数据。
 - 缺点：
   - 对异常值比较敏感。
   - 由于分类器之间存在依赖关系，所以很难进行并行计算。
  
##### XGBoost
 - 原理：XGBOOST是一种梯度提升的算法，用来解决分类和回归问题。它的基学习器可以是CART树，也可以是线性分类器。当用CART树做基学习器时，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、特征粒度上支持并行计算和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升。
 - XGBoost和GBDT的区别：
   - 传统的GBDT是以**CART树**作为基分类器，xgboost还**支持线性分类器**，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题），线性分类器的速度是比较快的，这时候xgboost的速度优势就体现了出来。
   - 传统的GBDT在优化时只使用**一阶导数**，而xgboost对损失函数做了**二阶泰勒展开**，同时用到了一阶和二阶导数，并且xgboost支持使用**自定义损失函数**，只要损失函数可一阶，二阶求导。
   - xgboost在损失函数里**加入了正则项**，用来**减小模型的方差**，**防止过拟合**，正则项里包含了树的叶节点的个数， 每个叶子节点上输出的score的L2模的平方和。
   - xgboost里有一个参数叫学习速率（learning_rate）， **xgboost在进行完一次迭代后，会将叶子节点的权重乘上学习速率**，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把learing_rate设置得小一点，然后迭代次数(n_estimators)设置得大一点。
   - xgboost借鉴了随机森林的原理，**支持行抽样(subsample)和列抽样(colsample_bytree,colsample_bylevel)**， 行抽样指的是随机森林里对数据集进行有放回抽样，列抽样指的是对特征进行随机选择，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
 - XGBoost如何寻找最优特征：xgboost在训练过程中给出各个特征的增益最评分，大增益的特征会被选出来作为分裂依据，从而记忆了每个特征在模型训练时的重要性，从根到叶子中间节点涉及某特征的次数作为该特征重要性排序。
 - 缺失值处理：xgboost为缺失值设定了默认的分裂方向，xgboost在树的构建过程中**选择能够最小化训练误差的方向作为默认的分裂方向**，即在训练时将缺失值划入左子树计算训练误差，再划入右子树计算训练误差，然后将缺失值划入误差小的方向。
 - 数据采样方式：xgboost属于boosting方法的一种，所以采样时样本是不放回的，因而每轮计算样本不重复，另外，xgboost**支持子采样**，每轮计算可以不使用全部的样本，以减少过拟合。另外一点是xgboost还**支持列采样**，每轮计算按百分比随机抽取一部分特征进行训练，既可以提高速度又能减少过拟合。
 - 调参步骤：
   - 保持learning rate和其他booster相关的参数不变，调节estimators的参数。learing_rate可设为0.1, max_depth设为4-6之间，min_child_weight设为1，subsample和colsample_bytree设为0.8 ，其他的参数都设为默认值即可。
   - 调节 max_depth 和 min_child_weight 参数，首先，我们先大范围地粗调参数，然后再小范围地微调。
   - gamma参数调优
   - subsample 和 colsample_bytree 调优
   - 正则化参数调优，选择L1正则化或者L2正则化
   - 缩小learning rate，得到最佳的learning rate值
 - XGBOOST特征重要性的输出原理：xgboost是用get_score方法输出特征重要性的，其中importance_type参数支持三种特征重要性的计算方法：
   - importance_type=weight（默认值），使用特征在所有树中作为划分属性的次数。
   - importance_type=gain，使用特征在作为划分属性时loss平均的降低量。
   - importance_type=cover，使用特征在作为划分属性时对样本的覆盖度。

##### LightGbm
 - LightGBM相比XGBOOST在原理和性能上的差异：
   1. 速度和内存上的优化：
      - xgboost用的是预排序（pre-sorted）的方法， 空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。
      - LightGBM用的是**直方图（Histogram）的决策树算法**，直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。
   2. 准确率上的优化：
      - xgboost 通过level（depth）-wise策略生长树， Level-wise过一次数据可以**同时分裂同一层的叶子**，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。
      - LightGBM通过leaf-wise（best-first）策略来生长树， Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，**找到分裂增益最大的一个叶子**，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。
   3. 对类别型特征的处理：
      - xgboost**不支持直接导入类别型变量**，需要预先对类别型变量作亚编码等处理。如果类别型特征较多，会导致哑变量处理后衍生后的特征过多，学习树会生长的非常不平衡，并且需要非常深的深度才能来达到较好的准确率。
      - LightGBM可以支持直接导入类别型变量（导入前需要将字符型转为整数型，并且需要声明类别型特征的字段名），它**没有对类别型特征进行独热编码**，因此速度比独热编码快得多。LightGBM使用了一个特殊的算法来确定属性特征的分割值。基本思想是**对类别按照与目标标签的相关性进行重排序**，具体一点是对于保存了类别特征的直方图根据其累计值(sum_gradient/sum_hessian)重排序,在排序好的直方图上选取最佳切分位置。

### 二、特征工程
 - 概念：特征工程指的是使用专业知识和技巧来处理数据，使得特征在机器学习算法上发挥更好的作用的过程。这个过程包含了数据预处理，特征构建，特征筛选等。特征工程的目的就是筛选出好的特征，得到更好的训练数据，使模型达到更好的效果。
 - 重要性：从数据中提取出来的特征好坏会直接影响到模型的效果，有的时候，如果特征工程做得好，仅使用一些简单的机器学习算法，也能达到很好的效果。由此可见特征工程在实际的机器学习中的重要性。
 - 步骤：
   - **数据获取**，数据的可用性评估（覆盖率，准确率，获取难度）
   - **探索性数据分析**，对数据和特征有一个大致的了解，同时进行数据的质量检验，包括缺失值，异常值，重复值，一致性，正确性等。
   - **特征处理**，包括数据预处理和特征转换两部分，数据预处理主要做清洗工作（缺失值，异常值，错误值，数据格式），特征转换即对连续特征，离散特征，时间序列特征进行转换，便于入模。
   - **特征构建**，特征构建的目的是找寻与目标变量相关且区分度较好的特征。常用的方法有特征交叉，四则运算，基于业务理解进行头脑风暴构建特征等。
   - **特征筛选**，大量的特征中选择少量的有用特征，也叫作特征降维，常用的方法有过滤法，包装法，嵌入法。
 - 特征工程迭代：
   - 选择特征：具体问题具体分析，通过查看大量的数据和基于对业务的理解，从数据中查找可以提出数据的关键。
   - 设计特征：可以自动进行特征提取工作，也可以手工进行特征的构建。
   - 选择特征：使用不同的特征构造方法，从多个角度来评判这个特征是否适合放入模型中。
   - 计算模型：计算模型在该特征上所提升的准确率。
   - 上线测试：通过在线测试的效果来评估特征是否有效。
 - 常用的特征工程：特征处理、特征构建、特征筛选。
 - 风控建模中的特征工程注意：可用性评估（相关性与真实性）、特征筛选（稳定性、覆盖率与可解释性）、对数据敏感性和业务理解。
 - 风控模型中的缺失值处理：根据缺失原因判断处理方式、考虑数据可用性。
 - 异常值处理：
   - 发现：基于统计异常点检测算法（极差、四分位数间距、均差、标准差）、通过距离方法检测异常点（欧氏距离、绝对距离）
   - 处理：先检测是否数据错误决定是否直接删除，不删除可使用评分卡的woe转换或将异常值直接归为一类。
 - 特征转换：
   - 时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者与位置变量进行结合衍生成新的特征。
   - 连续型特征：标准化，归一化，区间缩放，离散化。在评分卡中主要用的是离散化，离散化常用的方法有卡房分箱，决策树分箱，等频和等深分箱。
   - 离散型特征：如果类别数不是很多，适合做亚编码处理，对于无序离散变量用独热编码，有序离散变量用顺序编码。如果类别数较多，可用平均数编码的方法。
 - 样本不平衡问题：样本界定规则检查是否合理、扩大数据集是否缓解、数据集欠采样或过采样、更换模型适应数据（xgboost和lightgbm）、把小样本当作异常值作异常值检测。
 - 数据衍生常用方法：业务层面构造特征、特征交叉、分解类别特征（二值化、亚编码）、重构数值量（单位转换、整数小数拆分、构造阶段性特征）、特征的四则运算。
 - 特征筛选：
   - 目的：简化模型、增加可解释性、减低过拟合风险、缩短训练时间、避免维度灾难。
   - 特征要求：良好的区分能力、可解释性高、时间序列上稳定、覆盖率符合要求。
   - 方法：
     - Filter（过滤法）：根据相关性设定阈值选取特征。
       - 评分工具：相关系数，方差（适用于连续型变量），卡方检验（适用于类别型变量），信息熵，IV。实际工作中主要基于IV和相关性系数（皮尔逊系数）。
       - 优点：算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。
       - 缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。
     - Wrapper（封装法）：利用学习算法的性能评价特征子集的优劣，训练一个分类器，根据分类器的性能对该特征子集进行评价。
       - 具体方法：完全搜索（递归消除法），启发式搜索（前向/后向选择法，逐步选择法），随机搜索（训练不同的特征子集）。实际工作中主要用到启发式搜索，例如评分卡的逐步逻辑回归。
       - 优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。
       - 缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。
     - Embedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。
       - 具体方法：一种是基于惩罚项，例如岭回归，lasso回归，L1/L2正则化。另一种是基于树模型输出的特征重要性，在实际工作中较为常用，可选择的模型有随机森林，xgboost，lightgbm。
       - 优点：效果最好速度最快，模式单调，快速并且效果明显。
       - 缺点：如何参数设置， 需要对模型的算法原理有较好的理解。

### 三、模型评估和优化
 - 常用评价指标：
   - 混淆矩阵指标：精准率，查全率，假正率。当模型最后转化为规则时，一般用这三个指标来衡量规则的有效性。要么注重精准率，要么注重查全率，两者不可兼而得之。
   - ROC曲线和AUC值，ROC曲线是一种对于**查全率**（召回率）和**假正率**的权衡，具体方法是在不同阈值下以查全率作为纵轴，假正率作为横轴绘制出一条曲线。曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。**AUC是ROC曲线下面的面积**，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。在对角线（随机线）左边的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的预测概率排序，所以AUC反映的是分类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。
   - KS：用于区分预测正负样本分隔程度的评价指标，KS越大，表示模型能将好坏样本区分开的程度越大。KS的绘制方法是先将每个样本的预测结果化为概率或者分数，将最低分到最高分（分数越低，坏的概率越大）进行排序做样本划分，横轴就是样本的累计占比，纵轴则是好坏用户的累计占比分布曲线，KS值为两个分布的最大差值（绝对值）。**KS值仅能代表模型的区隔能力，KS不是越高越好**，KS如果过高，说明好坏样本分的过于开了，这样整体分数（概率）就是比较极端化的分布状态，这样的结果基本不能用。
   - 基尼系数：其横轴是根据分数（概率）由高到低累计的好用户占总的好用户的比例，纵轴是分数（概率）从高到低坏用户占总的坏用户的比例。由于分数高者为低风险用户，所以累计坏用户比例的增长速度会低于累计好用户比例，因此，基尼曲线会呈现向下弯曲的形式，向下突出的半月形的面积除以下方三角形的面积即是基尼系数。**基尼系数越大，表示模型对于好坏用户的区分能力越好。**
 - ROC适合不平衡数据：同时聚焦到正例与反例，数据分布不平衡时图像也不会产生很大变化。
 - AUC和KS的关系：（看图解释）
 - 欠拟合与过拟合：
   - 定义：
     - 欠拟合指的是模型没有很好的捕捉到数据特征，不能很好的拟合数据。
     - 过拟合指的是模型把数据学习的太彻底，以至于把噪声数据学习进去了，这样模型在预测未知数据时，就不能正确的分类，模型的泛化能力太差。
   - 如何判断：判断模型是否存在过拟合/欠拟合主要用学习曲线，当训练集和测试集的误差收敛但却很高时，即为欠拟合，当训练集和测试集的误差之间有大的差距时，为过拟合。
   - 解决：
     - 欠拟合：增加效果好的特征，添加多项式特征，减小正则化参数等。
     - 过拟合：使用更多的数据，选择更加合适的模型，加入正则项等。
 - 正则化：
   - 正则化是在模型的loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项，它会向学习算法略微做些修正，从而让模型能更好地泛化。这样反过来能提高模型在不可见数据上的性能。
   - L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解，所以L1正则化会趋向于产生少量的特征。
   - L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），所以L2正则化会使特征的解趋近于0，但不会为0。
 - 交叉验证：
   - 目的：评估给定算法在特定数据集上训练后的泛化性能，比单次划分训练集和测试集的方法更加稳定，全面。
   - 优点：交叉验证中每个样本都会出现在训练集和测试集中各一次，结果更加稳定，全面，具有说服力。
   - 常用方法：标准K折交叉验证、分层K折交叉验证（类别分层）、留一法交叉验证（适用于小数据量）、打乱划分交叉验证、分组交叉验证。
