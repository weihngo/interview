# 面试
## 1、比赛相关
### 语义分割比赛
#### deeplabv3发展历程
1. deeplabv1
    - 采用vgg16前向计算，减少了后两个池化层以减少图像下采样，为保证视野采用空洞卷积
    - 使用1x1卷积替换全连接层实现全卷积网络
    - 使用条件随机场使得边缘更清晰
    - 多尺寸预测，在每个池化层后面都加一层1x1卷积，类似图像金字塔结构（FCN），得到四个特征图作为预测。
2. deeplabv2
    - 受SPP启发的来ASPP，在最后采用类似inception的结构网络，多分支多样空洞卷积。
3. deeplabv3
    - 使用Multi-Grid策略，即在模型后端多家几层不同rate的空洞卷积，扩大感受野
    - 将batch归一化加入ASPP模块，使得数据训练更加有效
    - DeepLabv3 的 ASPP 加入了 全局池化层+conv1x1+双线性插值上采样 的模块 解决随着特征图的变小，空洞卷积退化为1x1卷积。
4. deeplabv3+
    - 将deeplabv3看成Encoder-Deconder结构
    - 借鉴MobileNet简化卷积
    - 使用修改过的Xception
#### 医学中Unet和Deeplabv3对比
相比于FCN和Deeplab等，UNet共进行了4次上采样，并在同一个stage使用了skip connection，而不是直接在高级语义特征上进行监督和loss反传，这样就保证了最后恢复出来的特征图融合了更多的low-level的feature，也使得不同scale的feature得到了的融合，从而可以进行多尺度预测和DeepSupervision。4次上采样也使得分割图恢复边缘等信息更加精细。

#### 语义分割loss
1. 交叉熵loss
2. 带权交叉熵loss
3. focal loss：解决样本数量不平衡，在二元交叉熵损失前面加参数a的基础上，将高置信度样本的损失降低一些，即动态a
4. Dice Loss：类似交并比结构，使用在样本极度不均衡的情况，如果一般情况下使用Dice Loss会回反向传播有不利的影响，使得训练不稳定。
5. IOU loss：与Dice loss类似，分母不同，略小。
6. Lovasz-Softmax Loss：


## 2、风控算法相关问题
### 算法
#### 1、逻辑回归
 - 优点：
   - 结构：实现简单、速度快、占用内存小、可在短时间内迭代多个版本的模型。
   - 可解释性：模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是逻辑回归模型。
   - 鲁棒性：模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。
   - 特征工程：特征工程做得好，模型的效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。
   - 可用性：模型的结果可以很方便的转化为策略规则，且线上部署简单。
 - 缺点：
   - 准确度：容易欠拟合，相比集成模型，准确度不是很高。
   - 数据处理困难：对数据的要求比较高，逻辑回归对缺失值，异常值，共线性都比较敏感，且不能直接处理非线性的特征。所以在数据清洗和特征工程上会花去很大部分的时间。
   - 场景适应能力差：在金融领域对场景的适应能力有局限性，例如数据不平衡问题，高维特征，大量多类特征，逻辑回归在这方面不如决策树适应能力强。
 - 逻辑回归与线性回归：逻辑回归是一种广义线性模型，它引入了Sigmod函数，是非线性模型。**逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量因变量关系进行拟合。**
 - 过拟合问题：减少特征数量、正则化。
 - 逻辑回归为什么要对特征进行离散化：
   - 特征离散化：将连续特征通过分箱等操作进行离散，使用逻辑回归计算
   - 特征交叉：将不同特征通过计算形成合成特征，有助于表示非线性关系。
   - 优点：简化模型、降低过拟合、异常值处理、计算迅速易于迭代、离散特征交叉引入非线性。
 - 评分卡为什么要进行woe化：更好的解释行，避免one-hot编码隔离了原始数据间的关系；用于筛选特征；可处理缺失值。
 - 剔除高度相关特征：高度相关特征无用、简化模型、防止过拟合、提高可解释性。
 - 特征系数并不能表示重要性，特征相关性越高，系数表示重要性越不准确。
 - 为什么用极大似然函数作为损失函数：在逻辑回归中，极大似然函数是凸函数，更易于算法求解，如梯度下降、牛顿法等。

#### 2、决策树
 - 优点：
   - 可理解：易于理解，决策树可以生成IF..TEHN逻辑表达的树结构，可解释性很好。
   - 数据处理少：相比逻辑回归对数据的处理较简单，不太需要做例如数据离散化，归一化等操作。
   - 非线性：决策树是目前已知的对于处理非线性交互的最好的算法。
   - 效果好：模型的效果比较好，例如随机森林，xgboost都是基于决策树构建的。
 - 缺点：
   - 过拟合
   - 不适合高维数据
   - 泛化能力差，特别对于没出现过的值
 - 决策树原理：基于树结构构建，过程为：特征选择、决策树生成、剪枝。
 - 简述ID3，C4.5，CART三类决策树的原理和异同点：
   - ID3选择最佳分割点是基于信息增益的，信息增益越大，表明使用这个属性来划分所获得的“纯度提升”越大。C4.5对ID3进行了改进，因为ID3使用的信息增益对数据划分时，可能出现每个结点只包含一个样本，这些子节点的纯度已经达到最大，但是，这样的决策树并不具有泛化能力，无法对新样本进行预测。且ID3不能处理连续型变量和缺失值。而C4.5使用信息增益率来选择属性，克服了信息增益选择属性时偏向选择值多的属性的不足。且可以处理连续型变量和缺失值。
   - C4.5是基于ID3的改进版，只能用于分类。而CART树既可以做分类，也可以做回归。CART的本质是对特征空间进行二元划分，所以**CART生成的是一颗二叉树**，且可以对类别型变量和数值型变量进行分裂。对分类型变量进行划分时，分为等于该属性和不等于该属性，在对连续型变量进行划分时，分为大于和小于，在做分类时使用的是**GINI系数**作为划分标准，在做回归时使用的是**均方误差**。
 - 分类树和回归树的区别：回归树的流程是类似分类树的，**区别在于划分时的标准不再是最大熵，而是最小化均差**，如果节点的预测值错的越离谱，均方差越大，通过最小化均差能够找到最可靠的分支依据。
 - 决策树对缺失值如何处理：
   - 考虑选择属性时：忽略；统一值填充；根据缺失值给增益率打折（正则化）。
   - 选择后确定分割点：忽略；统一值填充；通过预测算法将缺失个体分配到子集中；全部分配给所有子集；作为单独分支。
   - 测试集上的缺失：分配给最常见的值；终止分类并归属给子集中概率最高的。
 - 决策树不需要归一化：决策树是一种概率模型，所以不需要做归一化，因为它不关心变量的值，而是关心变量的分布和变量之间的条件概率，所以归一化这种数值缩放，不影响分裂结点位置。
 - 过拟合问题：预剪枝；后剪枝。

#### 3、集成学习
 - 集成学习主要内容：
   - 集成学习是一种优化手段和策略，通常是结合多个简单的弱分类器来集成模型组，去做更可靠的决策。一般的弱分类器可以是决策树，SVM，kNN等构成，其中的模型可以单独来训练，并且这些弱分类器以某种方式结合在一起去做出一个总体预测。集成学习就是找出哪些弱分类器可以结合在一起，以及如何结合的方法。目前集成学习主要有bagging，boosting，stacking三种：
     - bagging：**对训练集进行随机子抽样，对每个子训练集构建基模型**，对所有的基模型的预测结果进行综合产生最后的预测结果。如果是分类算法，则用**多数投票法**确定最终类别，如果是回归算法，则将各个回归结果做**算术平均**作为最终的预测值。常用的bagging算法：随机森林
     - boosting：训练过程为**阶梯状**，基模型按照次序进行训练（实际上可以做到并行处理），先给定一个初始训练数据，训练出第一个基模型，根据基模型的表现对样本进行调整，在之前基模型**预测错误的样本（残差）**上投入更多的关注，然后用调整后的样本训练下一个基模型，重复上述过程N次，将N个基模型进行加权结合，输出最后的结果。常用的算法有GBDT，XGBOOST等。
     - stacking：是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练(一般用**LR**进行回归组合），从而得到完整的stacking模型。要得到stacking模型，关键在于如何构造第二层的特征，**构造第二层特征的原则是尽可能的避免信息泄露**，因此对原始训练集常常采用类似于K折交叉验证的划分方法。各个基模型要采用相同的Kfold，这样得到的第二层特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。
 - 偏差与方差：**偏差**描述的是**预测值与真实值的差距**，偏差越大，越偏离真实数据；**方差**描述的是**预测值的变化范围，离散程度，方差越大，数据分布越分散**；bagging主要关注的是降低方差，boosting主要关注降低偏差。

##### 随机森林
 - 原理：随机森林是bagging算法的代表，使用了CART树作为弱分类器，将多个不同的决策树进行组合。随机森林会通过**自助采样的方法（bootstrap）得到N个训练集**，然后在单个训练集上会**随机选择一部分特征**，来选择一个最优特征来做决策树的左右子树划分，最后得到N棵决策树，对于分类问题，按**多数投票**的准则确定最终结果，对于回归问题，由多棵决策树的预测值的**平均数**作为最终结果。随机森林的随机性体现在两方面，一个是**选取样本的随机性**，一个是**选取特征的随机性**，这样进一步增强了模型的泛化能力。
 - 优点：
   - 并行：训练可以高度并行化，训练速度快，效率高，随机性也减少了数据训练和特征选择的开销。
   - 泛化：两个随机性的引入，使得随机森林不容易过拟合，具有很好的抗噪声能力。
   - 数据适应性：对于数据的适应能力强，可以处理连续型和离散型的变量，数据无需规范化。
   - 作为降维方法：可以输出变量的重要程度，被认为是一种不错的降维方法。
 - 缺点：
   - 噪音不适：在某些噪声较大的分类问题和或回归问题上容易过拟合。
   - 可解释性：模型的可解释性比较差，无法控制模型内部的运行。
   - 低维数据不友好：对于小数据或者低维数据，效果可能会不太好。
 - 随机森林如何输出特征重要性：
   - 随机森林对于特征重要性的评估思想：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是**基尼指数**或**袋外数据错误率**。

##### Adaboost
 - 原理：Adaboost基于分类器的错误率分配不同的权重系数，最后得到累加加权的的预测结果。
   - 流程
     - 给数据中每一个样本一个权重，若有N个样本，则每个样本的权重为1/N.
     - 训练数据的每一个样本，得到第一个分类器。
     - 计算该分类器的错误率，**根据错误率计算给分类器分配的权重**。
     - 将第一个分类器**分错的样本权重增加，分对的样本权重减少**，然后再用新的样本权重训练数据，得到新的分类器。
     - 迭代这个训练步骤直到分类器错误为0或达到迭代次数。
     - 将所有的**弱分类器加权求和**，得到分类结果（分类器权重），错误率低的分类器获得更高的决定系数，从而在数据进行预测起关键作用。
   - 优点：
     - 精度高，构造简单，结果可理解
     - 可以使用各种回归分类模型来构建弱学习器，非常灵活。
     - 不容易过拟合。
   - 缺点：
     - 训练时会过于偏向分类困难的数据，导致Adaboost容易受噪声数据干扰。
     - 依赖于弱分类器，训练时间可能比较长。

##### GBDT
 - 原理：
   - GBDT是boosting的一种方法，主要思想是每一次建立单个分类器时，是在之前建立的模型的**损失函数的梯度下降方向**。损失函数越大，说明模型越容易出错，如果我们的模型能让损失函数持续的下降，则说明我们的模型在持续不断的改进，而最好的方式就是让损失函数在其梯度的方向上下降。
   - GBDT的核心在于**每一棵树学的是之前所有树结论和的残差**，残差就是真实值与预测值的差值，所以为了得到残差，GBDT中的树全部是**回归树**，之所以不用分类树，是因为分类的结果相减是没有意义的。
   - Shrinkage（缩减）是 GBDT 的一个重要演进分支，Shrinkage的思想在于每次走一小步来逼近真实的结果，要比直接迈一大步的方式更好（**慢逼近**），这样做可以有效**减少过拟合的风险**。它认为每棵树只学到了一小部分，累加的时候只累加这一小部分，通过多学习几棵树来弥补不足。这累加的一小部分（步长*残差）来逐步逼近目标，所以各个树的残差是渐变的而不是陡变的。
   - GBDT可以用于回归问题（线性和非线性），也可用于分类问题。
 - 高维稀疏特征不适用GBDT：GBDT在每次分割时需要大量特征，训练耗时；树的分割往往只考虑了少部分特征，大部分的特征都用不到，所有的高维稀疏的特征会造成大量的特征浪费。
 - GBDT与随机森林：
   - 相同点：都是由多棵树构成，最终的结果也是由多棵树决定。
   - 不同点：
     - 随机森林可以由分类树和回归树组成，GBDT只能由回归树组成。
     - 随机森林的树可以并行生成，而GBDT只能串行生成，所以随机森林的训练速度相对较快。
     - 随机森林关注减小模型的方差，GBDT关注减小模型的偏差。
     - 随机森林对异常值不敏感，GBDT对异常值非常敏感。
     - 随机森林最终的结果是多数投票或简单平均，而GBDT是加权累计起来。
 - 优点：
   - GBDT每一次的残差计算都增大了分错样本的权重，而分对的权重都趋近于0，因此泛化性能比较好。
   - 可以灵活的处理各种类型的数据。
 - 缺点：
   - 对异常值比较敏感。
   - 由于分类器之间存在依赖关系，所以很难进行并行计算。
  
##### XGBoost