# 目标识别篇

## 1、FPN结构
- 不同特征图进行Add，浅层特征图通过1x1卷积改变通道。深层特征图通过上采用保证大小一致

## 2、RCNN系列
### R-CNN
- 步骤
  - 一张图像生成1k~2k个**候选区域**（使用selective search方法）
  - 对每一个候选区域，使用深度网络**提取特征**
  - 特征送入每一类的**SVM**分类器，判别是否属于该类
  - 使用回归精细修正候选框位置

### Fast R-CNN
- 步骤
  - 一张图像生成1k~2k个**候选区域**（使用selective search方法）
  - 将图像输入网络得到对应的特征图，将SS算法生成的候选框投影到特征图上获得相应的特征矩阵。
  - 将每个特征矩阵通过ROI pooling层缩放到7x7大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。

-  训练数据采样（加入正样本、负样本概念）。
-  ROI Pooling Layer：
   -  作用：提取预选框中的特征数据通过了ROI Pooling Layer缩放到统一尺寸，之后利用全连接进行分类和回归。
   -  实现：根据输入image，将ROI映射到feature map对应位置；将映射后的区域划分为相同大小的sections；对每个sections进行max pooling操作。
-  损失：
   -  分类损失：根据softmax之后的概率作交叉熵损失
   -  边界框回归损失：xywh的$Smooth_{L_1}$损失($|x|<1$时为$0.5x^2$，否则为$|x|-0.5$)。只有该候选区域属于正样本时才会计算。

### Faster R-CNN
- 步骤
  - 将图像输入网络得到相应的特征图
  - 使用RPN结构生成候选框， 将RPN生成的候选框投影到特征图上获得相应的特征举证。
  - 将每个特征矩阵通过ROI pooling层缩放到7x7大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果。

- RPN
  - 特征图上每个位置生成9个anchor（三尺寸、三比例）（20k），忽略跨越边界的anchor减少2/3。
  - 通过RPN将anchor调整到候选框的位置，采用非极大值抑制进一步减少，又减少2/3。
  - 特征图每个位置根据3x3大小的滑动窗口进行特征提取。
  - 可结合FPN优化使用

- 训练中大量anchor并不是都用来训练的，通过抽样得到256个训练样本，并使得正负样本比例为1:1，正样本不够用负样本来补充或减少负样本。
- 若真实框与所有预测框IOU都不足0.7，则选用IOU最大的作为正样本。

- RPN多分类损失函数：
  - 分类损失：多类别交叉熵损失（有softmax激活函数）
  - 边界框回归损失：xywh的$Smooth_{L_1}$损失

- Fast R-CNN损失与前面相同，定义两个损失可以采用RPN和Fast R-CNN分步训练方式，也可以进行联合训练。
  - two stage训练方式：
    1. 利用ImageNet与训练分类模型初始化前置卷积网络层参数，并开始单独训练RPN网络参数（前景框预选、边界框回归损失）；
    2. 股东RPN网络独有的卷积层以及全连接层参数，再利用ImageNet与训练分类模型初始化前置卷积网络参数，并利用RPN网络生成的目标建议框去训练Fast RCNN网络参数。
    3. 固定利用Fast RCNN训练好的全职卷积网络层参数，去微调RPN网络独有的卷积层以及全连接层。
    4. 同样保持固定 前置卷积网络层参数，去微调Fast RCNN网络的全连接参数。最后RPN网络与Fast RCNN网络共享前置卷积网络层参数，构成一个统一网络。

## 3、SSD

- Faster R-CNN存在的问题
  - 对小目标检测效果很差（卷积层太多，都是高层语义）
  - 模型大，检测速度较慢

- 规定图片输入大小
- 在不同特征尺度预测不同大小的目标
- 针对不同的大小预测不同尺寸的框，并带有多种比例。具体为第一和最后两个为4个框，中间三个为6个框，比例有：0.3、0.5、1、2、3，一共八千多个框。
- 预测类别为c+1，也预测背景框。
- 每个框只预测一套xywh，不会针对不同类别去预测专属框。
- 根据置信度差异判断负样本，不是选取全部负样本，大致3:1。

- 损失
  - 类别损失：正样本类别损失+负样本类别损失（softmax）
  - 定位损失：针对正样本的xywh的$Smooth_{L_1}$损失

## 4、yolo系列
### yolo v1
- 7x7 网格，每个网格预测两个框，输出为 7x7x30(5+5+20)，预测框为相对值。
- 置信度=P(目标) x IOU
- 分类置信度=P(类别)xP(目标)xIOU
- 损失函数：
  - 回归损失：误差平方和（中心平方差+长宽开根平方差）
  - 目标损失：误差平方和（正样本损失+负样本损失）
  - 类别损失：误差平方和（每个类别置信度平方差之和）

### yolo v2
- 与v1相比的改进
  - 每个卷积层后面添加归一化层，加速训练并提升map 2%，代替dropout。有归一化层之后无需bais。
  - 更大的输入尺寸，提升map 4%
  - 基于anchor预测，召回率大大提升。
  - 预选框聚类来获取anchor尺寸。
  - 对框内生成的预选框中心偏移量进行限制，保证预测框在自己的cell内，保证网络训练稳定。
  - 结合底层信息进行目标预测，26x26x512与13x13x1024合并后再进行预测。
  - 多尺度训练，如：288、352、416、448、544（32的倍数）。每迭代10batches就随机改变一次尺寸。
  - Darknet-19，19个卷积层。
  - 每个cell预测5个目标框，每个目标框有各自的类别。

### yolo v3
- Darknet-53，53个卷积层（带残差块），步长为2的卷积层代替池化层。
- 每个scale预测三个尺度：3x（4+1+80）
- 多尺度特征图拼接预测，最后有三个输出。
- 如果不是正样本，那么既没有边界回归损失，也没用类别损失，仅有目标损失。
- 每个目标都会分到一个正样本，如果目标置信度大于阈值但并非该目标的最大匹配预测，则直接抛弃，不属于正负样本；小于阈值的则为负样本。
- 类别置信度是通过sigmoid处理的，并非softmax，因此使用二值交叉熵作为损失。
- 损失函数：
  - 置信度损失=二值交叉熵损失
  - 类别损失=二值交叉熵损失（多类别互不干扰预测置信度，计算交叉熵并累加）
  - 定位损失=差值平方和（计算都是偏移差值和比例差值）

### yolo v3 SPP
- 数据增强：
  - mosaic图像增强：增加数据的多样性，增加目标个数，BN能一次性统计多张图片的参数（类似batchsize增大四倍）。
- SPP模块：通过不同尺度的最大池化（步长为1）并concat，实现不同尺度的特征融合。
- CIOU
  - IOU：能够更好反应重合程度，具有尺度不变性，但不相交是恒为0.
  - GIOU：在区间[-1,1]，$IOU-\frac{大矩形-并集}{大矩形}$，**预测框与真实框同高或同框时，退化为IOU，收敛慢**。
  - DIOU：在区间[-1,1]，$IOU-\frac{中心点距离}{大矩形对角线平方}$，可以直接最小化两个boxes的距离，收敛块。
  - CIOU：同时考虑重叠面积、中心距离、长宽比。$IOU-(\frac{中心点距离}{大矩形}+av)$，v考虑了长宽比，$a=\frac{v}{(1-IOU)+v}$

- Focal loss：针对正负样本不平衡问题
  - $FL(p_t)=-a_t(1-p_t)^rlog(p_t)$
  - 平衡正负样本的学习权重，a和r都是。
  - 缺点：易受噪声干扰，且超参调整难度大。

